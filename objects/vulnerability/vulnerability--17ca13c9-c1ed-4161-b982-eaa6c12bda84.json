{
    "type": "bundle",
    "id": "bundle--b6b03901-f5d6-41eb-ae47-23736943b179",
    "objects": [
        {
            "type": "vulnerability",
            "spec_version": "2.1",
            "id": "vulnerability--17ca13c9-c1ed-4161-b982-eaa6c12bda84",
            "created_by_ref": "identity--8ce3f695-d5a4-4dc8-9e93-a65af453a31a",
            "created": "2023-04-06T00:04:59.768128Z",
            "modified": "2023-04-06T00:04:59.768128Z",
            "name": "CVE-2023-29374",
            "description": "In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
            "external_references": [
                {
                    "source_name": "cve",
                    "external_id": "CVE-2023-29374"
                }
            ]
        }
    ]
}